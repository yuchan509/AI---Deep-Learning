{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# 그래프 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['figure.figsize'] = 20, 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Embedding, LSTM, Activation, Conv1D, MaxPooling1D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# 문장을 잘라줌.\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# GPU 사용 초기화 및 할당.\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>평점</th>\n",
       "      <th>평가글</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>재미있어요 마지막에는 슬퍼서 눙물</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>정말 재미있었음..ㅋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>안보면 후회합니다..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>스토리 연기 다 좋았어요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>진짜 배우분들 연기부터 음악까지.. 완벽했어요ㅠㅠ내일 시험인데 뿌리치고 엄마랑 영화...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     평점                                                평가글\n",
       "0  10.0                                 재미있어요 마지막에는 슬퍼서 눙물\n",
       "1  10.0                                        정말 재미있었음..ㅋ\n",
       "2  10.0                                        안보면 후회합니다..\n",
       "3  10.0                                      스토리 연기 다 좋았어요\n",
       "4  10.0  진짜 배우분들 연기부터 음악까지.. 완벽했어요ㅠㅠ내일 시험인데 뿌리치고 엄마랑 영화..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시드값 설정.\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 크롤링을 통한 네이버 영화 평점 및 평가글 등 데이터.\n",
    "df = pd.read_csv('data/naver_star_data.csv')\n",
    "df.head()\n",
    "\n",
    "# 평점 및 평가글만 가져옴.\n",
    "df2 = df[['평점', '평가글']]\n",
    "\n",
    "# 학습량이 너무 많아 우선 500개만 가져옴.\n",
    "df2 = df2.sample(n=500).reset_index(drop=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "평점      0\n",
       "평가글    13\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  결측치 확인.\n",
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "평점     0\n",
       "평가글    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 제거.\n",
    "df2.dropna(inplace=True)\n",
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    415\n",
       "0.0     72\n",
       "Name: 평점, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 글 인덱스를 평점을 기준으로 나눠서 추출.\n",
    "a1 = df2.query('평점<=5').index\n",
    "a2 = df2.query('평점> 5').index\n",
    "\n",
    "# 새로운 값 설정.(1 : 긍정, 0 : 부정)\n",
    "df2.loc[a1, '평점']=0\n",
    "df2.loc[a2, '평점']=1\n",
    "\n",
    "df2['평점'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 단어 개수 : 35개\n",
      "-------------------------------------\n",
      "[[   0    0    0 ...  566  567  568]\n",
      " [   0    0    0 ...    3  569  111]\n",
      " [   0    0    0 ...    0  570  571]\n",
      " ...\n",
      " [   0    0    0 ... 3338   94  152]\n",
      " [   0    0    0 ... 3341 3342 3343]\n",
      " [   0    0    0 ...  565 3360 3361]]\n",
      "-------------------------------------\n",
      "(340, 35)\n",
      "(340,)\n",
      "(147, 35)\n",
      "(147,)\n",
      "-------------------------------------\n",
      "단어 개수 : 3362개\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리.\n",
    "docs    = df2['평가글'].values\n",
    "classes = df2['평점'].values\n",
    "\n",
    "# 단어 사전 생성.\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "# token.word_index\n",
    "\n",
    "# 생성 단어사전 기준으로 단어들을 숫자로 변환.\n",
    "x = token.texts_to_sequences(docs)\n",
    "\n",
    "# 최대 단어 개수 구함.\n",
    "max_cnt = 0\n",
    "for c in x :\n",
    "    a1 = len(c)\n",
    "    if max_cnt < a1:\n",
    "        max_cnt = a1\n",
    "print(f'최대 단어 개수 : {max_cnt}개')              \n",
    "print('-'*37)\n",
    "\n",
    "# padding => 서로 길이가 다른 단어수(리스트)의 개수를 맞춰줌.\n",
    "# 부족한 부분은  0으로 채움\n",
    "pad_x = pad_sequences(x, max_cnt)\n",
    "print(pad_x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(pad_x, classes, test_size=0.3, stratify=classes)\n",
    "print('-'*37)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# 단어의 개수에 1을 더한 값 구함.\n",
    "word_size = len(token.word_index) + 1 \n",
    "print('-'*37)\n",
    "print(f'단어 개수 : {word_size}개')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         336200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 64)          32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 55)                26400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 56        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 394,720\n",
      "Trainable params: 394,720\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 설정.\n",
    "model = Sequential()\n",
    "\n",
    "# 데이터 압축.\n",
    "model.add( Embedding(word_size, 100) )\n",
    "\n",
    "# 모델 세부 설정.\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(64,5,padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(55))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/4 [==============================] - 3s 30ms/step - loss: 0.6596 - accuracy: 0.8240\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5379 - accuracy: 0.8522\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4264 - accuracy: 0.8515\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.4152 - accuracy: 0.8425\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3979 - accuracy: 0.8572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x250e7ff0ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 컴파일.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습.\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4257 - accuracy: 0.8503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8503401279449463"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확도 평가.\n",
    "# RNN만 사용했을 경우와 비교했을 때, 작은 학습수임에도 불구하고 좋은 성능을 보임.\n",
    "model.evaluate(x_test, y_test)[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
