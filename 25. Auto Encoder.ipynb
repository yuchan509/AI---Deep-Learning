{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# 그래프 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['figure.figsize'] = 20, 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Embedding, LSTM, Activation, Conv1D, MaxPooling1D\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import LeakyReLU, BatchNormalization, Reshape, Input, UpSampling2D\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# GPU 사용 초기화 및 할당.\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 오토인코더(Auto-Encoder) : 입력층과 출력층이 같은 구조.\n",
    "\n",
    "    * 인코더(encoder) : 인지 네트워크(recognition network)라고도 하며, 입력을 내부 표현으로 변환.\n",
    "    * 디코더(decoder) : 생성 네트워크(generative nework)라고도 하며, 내부 표현을 출력으로 변환.\n",
    "<br>\n",
    "<br>\n",
    "* < 특징 >\n",
    "* 정밀하지는 않지만 학습량이 작을 때 사용하기 좋음.\n",
    "* 글 등과 같은 선명할 필요없는 이미지에 적합.\n",
    "* 적어도 입력값에 대해서는 복원을 잘함.\n",
    "* 오토인코더(Auto-Encoder)는 실제로 입력 데이터의 Feature를 추출할 때 많이 사용.\n",
    "* 주로 차원축소(Dimension Reduction)에 사용.\n",
    "    * Network parameter 초기화, pre-training에 많이 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# seed 값 설정.\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# 데이터 불러오기.\n",
    "from keras.datasets import mnist\n",
    "(x_train, _), (x_test,_) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# 데이터 표준화.\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "x_test  = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오토인코더는 두 가지 단계( 인코딩 단계와 디코딩 단계)를 거쳐 입력값을 재구성함.\n",
    "# 생성자 모델 생성.\n",
    "AutoEncoder = Sequential()\n",
    "\n",
    "# 인코딩. => 입력값의 차원을 축소. = 데이터 압축을 의미.\n",
    "AutoEncoder.add(Conv2D(16, kernel_size=3, padding='same', input_shape=(28,28,1), activation='relu'))\n",
    "AutoEncoder.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "\n",
    "AutoEncoder.add(Conv2D(8, kernel_size=3, padding='same', activation='relu'))\n",
    "AutoEncoder.add(MaxPooling2D(pool_size=2, padding='same'))\n",
    "\n",
    "AutoEncoder.add(Conv2D(8, kernel_size=3, padding='same', activation='relu', strides=2))\n",
    "\n",
    "\n",
    "# 디코딩. => 인코딩한 압축 정보를 원본 데이터로 복원.\n",
    "AutoEncoder.add(Conv2D(8, kernel_size=3, padding='same', activation='relu'))\n",
    "AutoEncoder.add(UpSampling2D())\n",
    "\n",
    "AutoEncoder.add(Conv2D(8, kernel_size=3, padding='same', activation='relu'))\n",
    "AutoEncoder.add(UpSampling2D())\n",
    "\n",
    "AutoEncoder.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "AutoEncoder.add(UpSampling2D())\n",
    "\n",
    "AutoEncoder.add(Conv2D(1, kernel_size=3, padding='same', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 8, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 8, 8, 8)           584       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 16, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 16)        1168      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 28, 28, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 1)         145       \n",
      "=================================================================\n",
      "Total params: 4,385\n",
      "Trainable params: 4,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AutoEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "469/469 [==============================] - 15s 16ms/step - loss: 0.3146 - val_loss: 0.1358\n",
      "Epoch 2/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.1318 - val_loss: 0.1173\n",
      "Epoch 3/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.1160 - val_loss: 0.1086\n",
      "Epoch 4/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.1086 - val_loss: 0.1035\n",
      "Epoch 5/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1041 - val_loss: 0.1002\n",
      "Epoch 6/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.1005 - val_loss: 0.0973\n",
      "Epoch 7/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0980 - val_loss: 0.0952\n",
      "Epoch 8/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0958 - val_loss: 0.0941\n",
      "Epoch 9/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0942 - val_loss: 0.0922\n",
      "Epoch 10/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0931 - val_loss: 0.0907\n",
      "Epoch 11/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0914 - val_loss: 0.0908\n",
      "Epoch 12/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0907 - val_loss: 0.0894\n",
      "Epoch 13/50\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0899 - val_loss: 0.0884\n",
      "Epoch 14/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0891 - val_loss: 0.0875\n",
      "Epoch 15/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0884 - val_loss: 0.0871\n",
      "Epoch 16/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0879 - val_loss: 0.0866\n",
      "Epoch 17/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0874 - val_loss: 0.0860\n",
      "Epoch 18/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0870 - val_loss: 0.0855\n",
      "Epoch 19/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0866 - val_loss: 0.0855\n",
      "Epoch 20/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0862 - val_loss: 0.0850\n",
      "Epoch 21/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0859 - val_loss: 0.0846\n",
      "Epoch 22/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0854 - val_loss: 0.0843\n",
      "Epoch 23/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0855 - val_loss: 0.0840\n",
      "Epoch 24/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0850 - val_loss: 0.0837\n",
      "Epoch 25/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0848 - val_loss: 0.0835\n",
      "Epoch 26/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0846 - val_loss: 0.0832\n",
      "Epoch 27/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0842 - val_loss: 0.0834\n",
      "Epoch 28/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0841 - val_loss: 0.0829\n",
      "Epoch 29/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0839 - val_loss: 0.0825\n",
      "Epoch 30/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0835 - val_loss: 0.0826\n",
      "Epoch 31/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0834 - val_loss: 0.0822\n",
      "Epoch 32/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0833 - val_loss: 0.0820\n",
      "Epoch 33/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0830 - val_loss: 0.0821\n",
      "Epoch 34/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0828 - val_loss: 0.0821\n",
      "Epoch 35/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0828 - val_loss: 0.0816\n",
      "Epoch 36/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0824 - val_loss: 0.0816\n",
      "Epoch 37/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0826 - val_loss: 0.0815\n",
      "Epoch 38/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0824 - val_loss: 0.0813\n",
      "Epoch 39/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0823 - val_loss: 0.0812\n",
      "Epoch 40/50\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0824 - val_loss: 0.0813\n",
      "Epoch 41/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0821 - val_loss: 0.0810\n",
      "Epoch 42/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0821 - val_loss: 0.0809\n",
      "Epoch 43/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0820 - val_loss: 0.0808\n",
      "Epoch 44/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0817 - val_loss: 0.0809\n",
      "Epoch 45/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0817 - val_loss: 0.0806\n",
      "Epoch 46/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0817 - val_loss: 0.0806\n",
      "Epoch 47/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0816 - val_loss: 0.0809\n",
      "Epoch 48/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0818 - val_loss: 0.0806\n",
      "Epoch 49/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0813 - val_loss: 0.0804\n",
      "Epoch 50/50\n",
      "469/469 [==============================] - 6s 14ms/step - loss: 0.0814 - val_loss: 0.0804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ca9f8d6ac8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습.\n",
    "AutoEncoder.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "AutoEncoder.fit(x_train, x_train, validation_data=(x_test, x_test), epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 무작위로 5개 이미지 추출.\n",
    "random_test = np.random.randint(x_test.shape[0], size=5)\n",
    "\n",
    "# 예측 이미지 생성.\n",
    "AE_img = AutoEncoder.predict(x_test)\n",
    "\n",
    "# 출력될 이미지의 크기.\n",
    "plt.figure(figsize=(7, 2))\n",
    "for i, img_idx in enumerate(random_test):\n",
    "    \n",
    "    # 원본 이미지.\n",
    "    ax = plt.subplot(2, 7, i+1)\n",
    "    plt.imshow(x_test[img_idx].reshape(28,28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 예측 이미지.\n",
    "    ax = plt.subplot(2, 7, 7+i+1)\n",
    "    plt.imshow(AE_img[img_idx].reshape(28,28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
